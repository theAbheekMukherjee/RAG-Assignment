{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theAbheekMukherjee/RAG-Assignment/blob/main/RAG(Assignment).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al0NfHfa-bAi"
      },
      "source": [
        "#RAG System utilising Llama Index\n",
        "This code will demonstrate how to construct a comprehensive RAG pipeline utilising the **Llama Index**.\n",
        "We will locally deploy a small-scale, open-source LLM called **Phi-3**, *developed by Microsoft*, on our Colab instance for the purpose of generation. In order to enhance the outcomes, we will grant the LLM access to Brochure Data, i.e., ** 2025 brochure available as Postgraduate 2025\n",
        "brochure.pdf on my.wbs**, which will be stored in Chroma database as a vector store. Ultimately, we employ **[Ollama]**(https://ollama.com/) as a means to engage with Phi-3 on our device."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YAZ80WZ_xeQ"
      },
      "source": [
        "Initially,\n",
        "We will start by installing all the packages necessary to interact with LLM (in our case Phi-3) and perform efficient information retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-L0DBm0Q-W6m"
      },
      "outputs": [],
      "source": [
        "# Installing prerequistes for interacting with LLMs, chunking and embedding\n",
        "!pip install llama_index.core                       #for core components of Llama_Index\n",
        "!pip install llama_index.readers.file\n",
        "!pip install faiss-gpu                              #installing separate library for faiss\n",
        "!pip install llama-index-embeddings-huggingface     #for generating embeddings\n",
        "!pip install llama-index-vector-stores-faiss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE0QbqmeE3js"
      },
      "source": [
        "**Step 1:**\n",
        "Prepares the data from a file, after loading into the directory, for further processing by splitting it into smaller, manageable chunks based on sentence splitter chunking technique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EsmSzo4o2_8l"
      },
      "outputs": [],
      "source": [
        "from llama_index.readers.file import FlatReader, PDFReader\n",
        "from llama_index.core.node_parser import SentenceSplitter #import sentence splitter for chunking\n",
        "from pathlib import Path  #for finding the file\n",
        "\n",
        "Brochure_doc = PDFReader().load_data(Path(\"/content/Postgraduate 2025 brochure--FINAL-ONLINE.pdf\")) #loading the dataset\n",
        "\n",
        "parser = SentenceSplitter(chunk_size=300, chunk_overlap=100) #Chunk size and chunk overlap can be changed - type of hyperparameter\n",
        "Brochure_doc = parser.get_nodes_from_documents(Brochure_doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i0b7At-JXca"
      },
      "source": [
        "**Step 2:**\n",
        "Configuring *LlamaIndex* to leverage a specific pre-trained model from Hugging Face ***(\"BAAI/bge-small-en-v1.5\")***  for the task of generating numerical representations ***(embeddings)*** from text data, i.e. * Postgraduate 2025\n",
        "brochure.pdf*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "r3pU-gJiBzCh"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "\n",
        "# Initialize a HuggingFace Embedding model\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\") #pre-trained model to understand chunking output\n",
        "\n",
        "# Specify the embedding model into LlamaIndex's settings\n",
        "Settings.embed_model = embed_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOE8QW3XDYh0"
      },
      "source": [
        "**Step 2a:**\n",
        "Setting up the Faiss index itself, which is a separate library for efficient similarity search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "348debb7"
      },
      "source": [
        "!pip install faiss-cpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WeitHaJSDVcO"
      },
      "outputs": [],
      "source": [
        "import faiss #used for similarity search in high-dimensional vector spaces\n",
        "# create the empty Faiss database\n",
        "d = 384  #embedding dimensions representing dimensionality - a hyperparameter\n",
        "faiss_index = faiss.IndexFlatIP(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4D7Ul8JORbD"
      },
      "source": [
        "**Step 3:**\n",
        "Integrating Faiss into LlamaIndex. It builds upon the Faiss index created earlier and integrates it within the LlamaIndex framework for creating a searchable data structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVcp19RcDwu9"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import (\n",
        "    load_index_from_storage,\n",
        "    VectorStoreIndex,\n",
        "    StorageContext,\n",
        ")\n",
        "\n",
        "from llama_index.vector_stores.faiss import FaissVectorStore\n",
        "\n",
        "# create a vector store variable\n",
        "vector_store = FaissVectorStore(faiss_index=faiss_index) #creating the vector store\n",
        "\n",
        "# set the vector database into the storage context of LlamaIndex\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# create the Faiss database\n",
        "li_index = VectorStoreIndex(Brochure_doc, storage_context=storage_context)\n",
        "\n",
        "# save index to disk\n",
        "li_index.storage_context.persist()\n",
        "\n",
        "print(f\"Number of vectors in the Faiss index: {faiss_index.ntotal}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d01o9_rPGVye"
      },
      "source": [
        "**Step 4:**\n",
        "We used *Llama Index* to prepare the question for searchable index interaction in previous steps. We then compare the question's meaning to the stored text data in our embedding space by transforming it into a vector using the *same pre-trained model* as the index. This lets the index find Brochure texts semantically equivalent to the Q & A prompt.\n",
        "Subsequently, we then pick the most relevant sentences that answer the question based on embedding space similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ukYdmTZzFTap"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Instantiate the sentence-level DistilBERT\n",
        "model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "# Q & A prompt-to store as a text string\n",
        "qna_prompt = \"Where is Warwick Business School?\"\n",
        "\n",
        "# Convert Q&A prompt to vectors\n",
        "rag_embedding = model.encode(qna_prompt, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atx3BOW6UoZ3"
      },
      "source": [
        "**Step 4a:**\n",
        "Choosing the most relevant sentences that directly address the question by considering their similarity in the embedding space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLsFMS_ZGnQZ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Retrieve the top nearest neighbour\n",
        "cs_similarity, similar = faiss_index.search(np.array([rag_embedding]), k=4) #performing search operation, where k=4 is number of neighbours to be identified(hyperparameter)\n",
        "similar = similar.flatten().tolist()\n",
        "\n",
        "# Print the indices of the four most similar passages\n",
        "print(f'Top results: {similar}')\n",
        "\n",
        "#printing the results\n",
        "for result in similar:\n",
        "  print(Brochure_doc[result]) # some results are not very good\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJGVDpXuCZFb"
      },
      "source": [
        "**Step 5:**\n",
        "**Putting everything together and building a RAG.**\n",
        "\n",
        "We'll start by installing ***Ollama***, setting up the model, and finally checking its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTvwu7CUXmU8"
      },
      "source": [
        "**Step 5a:**\n",
        "Installing the *Ollama*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gKNOzQvRA7V1"
      },
      "outputs": [],
      "source": [
        "# Install Ollama v0.1.30\n",
        "!curl https://ollama.ai/install.sh | sed 's#https://ollama.ai/download#https://github.com/jmorganca/ollama/releases/download/v0.1.30#' | sh\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHGuH7jBDntI"
      },
      "source": [
        "**Step 5b:**\n",
        "Next, we perform some tasks to set up *Ollama* in the background of our Colab (Linux) instance. We donâ€™t have to worry too much about this code; it mainly consists of Linux/BASH commands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAFCaMFJC2u2"
      },
      "outputs": [],
      "source": [
        "# Setup the model as a global variable\n",
        "OLLAMA_MODEL='phi:latest'\n",
        "\n",
        "# Add the model to the environment of the operating system\n",
        "import os\n",
        "os.environ['OLLAMA_MODEL'] = OLLAMA_MODEL\n",
        "!echo $OLLAMA_MODEL # print the global variable to check it saved\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Start ollama on the server (\"serve\") in the background\n",
        "command = \"nohup ollama serve &\"\n",
        "\n",
        "# Use subprocess.Popen to run the command\n",
        "process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "print(\"Process ID:\", process.pid) # print the process ID\n",
        "time.sleep(10)  # Increased wait time to 10 seconds to allow the server to initialize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaktZmv6DDdK"
      },
      "source": [
        "**Step 5c:**\n",
        "Now that everything is setup, we can query the model to generate some text about the Brochure. As an pre-trained LLM, we can check the outcome, which can be used to compare performance of our model later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21WIW2C0DCaP"
      },
      "outputs": [],
      "source": [
        "# Query the model via the command line\n",
        "# First time running it will \"pull\" (import) the model\n",
        "!ollama run $OLLAMA_MODEL \"What is the location of Warwick Business School?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3yJfG2GD1RU"
      },
      "source": [
        "**Step 6:**\n",
        "As everything works well, we can now build our RAG. Firstly, we start with preparing the environment for using Llama Index with Ollama.\n",
        "This allows us to *leverage strength of LLMs with text indexing and search functionalities.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tvSi7vWnD8HZ"
      },
      "outputs": [],
      "source": [
        "# Libraries that needs to be downloaded\n",
        "# Install prerequisites\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "!pip install llama-index-llms-ollama               #integration of ollama with LLM\n",
        "!pip install llama-index ipywidgets\n",
        "!pip install llama-index-llms-huggingface\n",
        "\n",
        "# Uninstall conflicting opentelemetry packages\n",
        "!pip uninstall -y opentelemetry-api opentelemetry-sdk\n",
        "\n",
        "# Install compatible opentelemetry versions\n",
        "!pip install opentelemetry-api==1.20.0 opentelemetry-sdk==1.20.0\n",
        "\n",
        "# Access to chroma vector store for efficient data storage\n",
        "!pip install llama-index-vector-stores-chroma      #access to chroma vector store for efficient data storage\n",
        "!pip install chromadb\n",
        "\n",
        "\n",
        "# Import required modules from the llama_index library\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.ollama import Ollama\n",
        "from llama_index.core import StorageContext\n",
        "\n",
        "# Import ChromaVectorStore and chromadb module\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "import chromadb\n",
        "\n",
        "# Import the Ollama class\n",
        "from llama_index.llms.ollama import Ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE4H-NOsdR3o"
      },
      "source": [
        "**Step 6a:**\n",
        "Importing Ollama and setting a timeout to raise an error. And, further integration of Ollama with LlamaIndex."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk3lH7YLEe1P"
      },
      "outputs": [],
      "source": [
        "# Use the global variable (OLLAMA_MODEL) as our LLM\n",
        "# Set a timeout of 4 minutes\n",
        "OLLAMA_MODEL='phi:latest' # Define OLLAMA_MODEL in this cell as a workaround\n",
        "llm = Ollama(model=OLLAMA_MODEL, request_timeout=480.0) # Increased timeout to 8 minutes\n",
        "\n",
        "# Specify the LLM and embedding model into LlamaIndex's settings\n",
        "Settings.llm = llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqFZfrEuFY4N"
      },
      "source": [
        "**Step 7:**\n",
        "Creation of a reusable ***Prompt Template*** for using Ollama (or any integrated LLM) within LlamaIndex for question answering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O92UbYKxFlLT"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.llms import ChatMessage, MessageRole\n",
        "from llama_index.core import ChatPromptTemplate\n",
        "\n",
        "qa_prompt_str = (\n",
        "    \"Context information is below.\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"Given the context information and not prior knowledge, \"\n",
        "    \"answer the question: {query_str}\\n\"\n",
        ")\n",
        "\n",
        "# Text QA Prompt\n",
        "chat_text_qa_msgs = [\n",
        "    ChatMessage(\n",
        "        role=MessageRole.SYSTEM,\n",
        "        content=(\n",
        "            \"Always answer the question,even if the context is limited.\"\n",
        "        ),\n",
        "    ),\n",
        "    ChatMessage(role=MessageRole.USER, content=qa_prompt_str),\n",
        "]\n",
        "\n",
        "text_qa_template = ChatPromptTemplate(chat_text_qa_msgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q1hAsw-Fw1Q"
      },
      "source": [
        "**Step 8:**\n",
        "Demonstrating how to use the configured LLM and prompt template within a temporary LlamaIndex query engine to answer the question asked. Now, we can compare the output of RAG and the pre-trained LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8N8iQWjia7R"
      },
      "outputs": [],
      "source": [
        "query_engine = li_index.as_query_engine(\n",
        "                                    text_qa_template=text_qa_template,\n",
        "                                    llm=llm,\n",
        "                                    response_mode=\"compact\")\n",
        "\n",
        "response = query_engine.query(\"What is the location of Warwick Business School?\")\n",
        "response.response"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this code cell if Ollama takes too long to establish connection"
      ],
      "metadata": {
        "id": "ZTSovwFGwNky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Stop any running Ollama processes\n",
        "!pkill ollama || true\n",
        "time.sleep(5) # Give it a moment to stop\n",
        "\n",
        "# Start ollama on the server (\"serve\") in the background\n",
        "command = \"nohup ollama serve &\"\n",
        "\n",
        "# Use subprocess.Popen to run the command\n",
        "process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "print(\"Ollama server process ID:\", process.pid) # print the process ID\n",
        "time.sleep(10)  # Wait for the server to initialize\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "IC5MDteTr6i_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = li_index.as_query_engine(\n",
        "                                    text_qa_template=text_qa_template,\n",
        "                                    llm=llm,\n",
        "                                    response_mode=\"tree_summarize\")\n",
        "\n",
        "response = query_engine.query(\"What is the location of Warwick Business School?\")\n",
        "response.response"
      ],
      "metadata": {
        "id": "QOSfIe97vj-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1GH8C2gOvuuG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bDVdUwBFqm7"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    li_index.as_query_engine(                 #creating a temporary query engine\n",
        "        text_qa_template=text_qa_template,\n",
        "        llm=llm,\n",
        "        response_mode=\"tree_summarize\"        #various types of response modes are available(a hyperparameter)\n",
        "    ).query(\"Where is the nearest study library in the campus from Warwick Business School?\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXgiu-0dkwR1"
      },
      "source": [
        "**Step 9:**\n",
        "Checking the outcome under different query modes and with different prompts to verify the model's performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2CyyydxjPvb"
      },
      "source": [
        "We can change response modes of query engine to see the change in the outcomes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YYCpxH8joW0"
      },
      "outputs": [],
      "source": [
        "query_engine = li_index.as_query_engine(\n",
        "                                    text_qa_template=text_qa_template,\n",
        "                                    llm=llm,\n",
        "                                    response_mode=\"compact\")\n",
        "\n",
        "response = query_engine.query(\"What is the eligibility to get into criteria in Warwick Business School?\")\n",
        "response.response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5rimV4SkRRi"
      },
      "source": [
        "Now we can see by selecting the \"Tree Summarize\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0Q_yIK0j74X"
      },
      "outputs": [],
      "source": [
        "query_engine = li_index.as_query_engine(\n",
        "                                    text_qa_template=text_qa_template,\n",
        "                                    llm=llm,\n",
        "                                    response_mode=\"tree_summarize\")\n",
        "\n",
        "response = query_engine.query(\"What is the eligibility to get into criteria in Warwick Business School?\")\n",
        "response.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rKXTuW4fFs4"
      },
      "outputs": [],
      "source": [
        "query_engine = li_index.as_query_engine(\n",
        "                                    text_qa_template=text_qa_template,\n",
        "                                    llm=llm,\n",
        "                                    response_mode=\"compact\")\n",
        "\n",
        "response = query_engine.query(\"How is the course Msc Business Analytics course taught?\")\n",
        "response.response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = li_index.as_query_engine(\n",
        "                                    text_qa_template=text_qa_template,\n",
        "                                    llm=llm,\n",
        "                                    response_mode=\"tree_summarize\")\n",
        "\n",
        "response = query_engine.query(\"How is the course Msc Business Analytics course taught?\")\n",
        "response.response"
      ],
      "metadata": {
        "id": "x8tVeiITZaLa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}